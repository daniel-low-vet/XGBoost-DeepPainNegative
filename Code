import numpy as np
import pandas as pd
import joblib
import shap
import matplotlib.pyplot as plt
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, make_scorer, roc_curve
from sklearn.utils import resample
import optuna

# Step 1: Load dataset
data_path = 'DEFINE DATA PATH'
data = pd.read_csv(data_path)

# Step 2: Separate features and target
X = data.drop(columns=['target', 'id'])
y = data['target']

# Step 3: Perform train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)

# Step 4: Define Optuna trial
def objective(trial):
    # Define the hyperparameter search space
    params = {
        'objective': 'binary:logistic',
        'eval_metric': 'logloss',
        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),
        'max_depth': trial.suggest_int('max_depth', 1, 10),
        'n_estimators': trial.suggest_int('n_estimators', 100, 5000),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)
    }

    # Train the XGBoost model
    model = xgb.XGBClassifier(**params)
    model.fit(X_train, y_train)

    # Predict on the validation set and calculate AUC
    y_pred = model.predict(X_test)
    auc = roc_auc_score(y_test, y_pred)
    return auc

# Step 5: Run Optuna study
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=100)

# Step 6: Print best hyperparameters
print("Best hyperparameters:", study.best_params)

# Step 7: Train the final model with the best hyperparameters
best_params = study.best_params
best_model = xgb.XGBClassifier(**best_params)
best_model.fit(X_train, y_train)

# Step 8: Evaluate the model on the test set
y_pred = best_model.predict(X_test)
y_pred_proba = best_model.predict_proba(X_test)[:, 1]

# Step 9: Calculate the clinical metrics
auc = roc_auc_score(y_test, y_pred_proba)
accuracy = accuracy_score(y_test, y_pred)
tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
sensitivity = tp / (tp + fn)
specificity = tn / (tn + fp)
ppv = tp / (tp + fp)
npv = tn / (tn + fn)

print("AUC:", auc)
print("Accuracy:", accuracy)
print("Sensitivity:", sensitivity)
print("Specificity:", specificity)
print("PPV:", ppv)
print("NPV:", npv)

# Set the number of bootstrap samples
n_bootstraps = 1000
random_state = 42

# Initialize lists to store AUC and accuracy for each bootstrap sample
bootstrapped_aucs = []
bootstrapped_accuracies = []

# Step 10: Perform bootstrapping
for i in range(n_bootstraps):
    # Resample the test set with replacement
    X_resampled, y_resampled = resample(X_test, y_test, random_state=random_state + i)

    # Make predictions on the resampled data
    y_pred_resampled = best_model.predict(X_resampled)
    y_pred_proba_resampled = best_model.predict_proba(X_resampled)[:, 1]

    # Calculate AUC and accuracy for the resampled data
    auc = roc_auc_score(y_resampled, y_pred_proba_resampled)
    accuracy = accuracy_score(y_resampled, y_pred_resampled)

    # Store the results
    bootstrapped_aucs.append(auc)
    bootstrapped_accuracies.append(accuracy)

# Calculate the 95% CI for AUC
auc_lower = np.percentile(bootstrapped_aucs, 2.5)
auc_upper = np.percentile(bootstrapped_aucs, 97.5)

# Calculate the 95% CI for accuracy
accuracy_lower = np.percentile(bootstrapped_accuracies, 2.5)
accuracy_upper = np.percentile(bootstrapped_accuracies, 97.5)

print("AUC 95% CI:", (auc_lower, auc_upper))
print("Accuracy 95% CI:", (accuracy_lower, accuracy_upper))

# Step 11: Define the features to withhold
withheld_features = ['time_to_surgery', 'cat_to_surgery_S2', 'cat_to_surgery_S3', 'procedure_mini_hemi', 'spaces_one', 'spaces_two', 'fenestration_none', 'fenestration_same_level', 'durotomy', 'intraop_comp', 'surgery_time', 'ga_time', 'hosp_time', 'followup_time', 'followup_method_remote']

# Step 12: Duplicate test set with selected features withheld
X_test_withheld = X_test.copy()
X_test_withheld[withheld_features] = np.nan

# Step 13: Evaluate model on modified test set with withheld features
y_pred_withheld = best_model.predict(X_test_withheld)
y_pred_proba_withheld = best_model.predict_proba(X_test_withheld)[:, 1]

# Step 14: Calculate performance metrics
auc_withheld = roc_auc_score(y_test, y_pred_proba_withheld)
accuracy_withheld = accuracy_score(y_test, y_pred_withheld)
tn, fp, fn, tp = confusion_matrix(y_test, y_pred_withheld).ravel()
sensitivity_withheld = tp / (tp + fn)
specificity_withheld = tn / (tn + fp)
ppv_withheld = tp / (tp + fp)
npv_withheld = tn / (tn + fn)

print("Withheld Feature Set Metrics:")
print("AUC:", auc_withheld)
print("Accuracy:", accuracy_withheld)
print("Sensitivity:", sensitivity_withheld)
print("Specificity:", specificity_withheld)
print("PPV:", ppv_withheld)
print("NPV:", npv_withheld)

# Set the number of bootstrap samples
n_bootstraps = 1000
random_state = 42

# Initialize lists to store metrics for each bootstrap sample
bootstrapped_aucs_withheld = []
bootstrapped_accuracies_withheld = []

# Step 15: Perform bootstrapping for withheld evaluation
for i in range(n_bootstraps):
    # Resample the test set with replacement
    X_resampled, y_resampled = resample(X_test_withheld, y_test, random_state=random_state + i)

    # Make predictions on the resampled data
    y_pred_resampled = best_model.predict(X_resampled)
    y_pred_proba_resampled = best_model.predict_proba(X_resampled)[:, 1]

    # Calculate AUC and accuracy for the resampled data
    auc = roc_auc_score(y_resampled, y_pred_proba_resampled)
    accuracy = accuracy_score(y_resampled, y_pred_resampled)

    # Store the results
    bootstrapped_aucs_withheld.append(auc)
    bootstrapped_accuracies_withheld.append(accuracy)

# Calculate the 95% CI for AUC and Accuracy
auc_lower = np.percentile(bootstrapped_aucs_withheld, 2.5)
auc_upper = np.percentile(bootstrapped_aucs_withheld, 97.5)
accuracy_lower = np.percentile(bootstrapped_accuracies_withheld, 2.5)
accuracy_upper = np.percentile(bootstrapped_accuracies_withheld, 97.5)

print("Withheld Feature Set AUC 95% CI:", (auc_lower, auc_upper))
print("Withheld Feature Set Accuracy 95% CI:", (accuracy_lower, accuracy_upper))

# Step 16: Create a SHAP explainer and calculate SHAP values
explainer_xgb = shap.Explainer(best_model, X_train)
shap_values_xgb = explainer_xgb(X_test)

# Step 17: Plot SHAP summary plot
shap.summary_plot(shap_values_xgb, X_test)
